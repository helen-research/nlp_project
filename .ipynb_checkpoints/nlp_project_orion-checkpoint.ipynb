{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLAN\n",
    "\n",
    "- [ ] Acquisition\n",
    "    - [ ] Select what list of repos to scrape.\n",
    "    - [ ] Get requests form the site.\n",
    "    - [ ] Save responses to csv.\n",
    "- [ ] Preparation\n",
    "    - [ ] Prepare the data for analysis.\n",
    "- [ ] Exploration\n",
    "    - [ ] Answer the following prompts:\n",
    "        - [ ] What are the most common words in READMEs?\n",
    "        - [ ] What does the distribution of IDFs look like for the most common words?\n",
    "        - [ ] Does the length of the README vary by language?\n",
    "        - [ ] Do different languages use a different number of unique words?\n",
    "- [ ] Modeling\n",
    "    - [ ] Transform the data for machine learning; use language to predict.\n",
    "    - [ ] Fit several models using different text repressentations.\n",
    "    - [ ] Build a function that will take in the text of a README file, and makes a prediction of language.\n",
    "- [ ] Delivery\n",
    "    - [ ] Github repo\n",
    "        - [x] This notebook.\n",
    "        - [ ] Documentation within the notebook.\n",
    "        - [ ] README file in the repo.\n",
    "        - [ ] Python scripts if applicable.\n",
    "    - [ ] Google Slides\n",
    "        - [ ] 1-2 slides only summarizing analysis.\n",
    "        - [ ] Visualizations are labeled.\n",
    "        - [ ] Geared for the general audience.\n",
    "        - [ ] Share link @ readme file and/or classroom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "from requests import get\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "BASEURL = 'https://github.com/search?p=1&q=stars%3A%3E0&s=stars&type=Repositories'\n",
    "HEADERS = {'User-Agent': 'Sentient Attack Helicoptor'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACQUIRE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing that needs to happen is to get the links from the most starred github repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_list(page):\n",
    "    urls = []\n",
    "    response = get(BASEURL, headers=HEADERS)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    max_page = page + 1\n",
    "    for i in range(1,max_page):\n",
    "        url = 'https://github.com/search?p=' + str(i) + '&q=stars%3A%3E0&s=stars&type=Repositories'\n",
    "        print(f'traversing url: {url}')\n",
    "        response = get(url, headers=HEADERS)\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        list_of_repos = soup.find('ul', class_='repo-list')\n",
    "        repository = list_of_repos.find_all('li', class_='repo-list-item')\n",
    "        for h in repository:\n",
    "            if h.find(attrs={'itemprop':'programmingLanguage'}):\n",
    "                a = h.find('a')\n",
    "                urls.append(a.attrs['href'])\n",
    "            else:\n",
    "                print(f'skipping because no programming language')\n",
    "    print(f'Scrapedf a total of {len(urls)} github urls.')\n",
    "    urls = ['https://github.com' + url for url in urls]\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traversing url: https://github.com/search?p=1&q=stars%3A%3E0&s=stars&type=Repositories\n",
      "skipping because no programming language\n",
      "skipping because no programming language\n",
      "skipping because no programming language\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://github.com/freeCodeCamp/freeCodeCamp',\n",
       " 'https://github.com/996icu/996.ICU',\n",
       " 'https://github.com/vuejs/vue',\n",
       " 'https://github.com/twbs/bootstrap',\n",
       " 'https://github.com/facebook/react',\n",
       " 'https://github.com/tensorflow/tensorflow',\n",
       " 'https://github.com/robbyrussell/oh-my-zsh']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_url_list(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct a url list\n",
    "\n",
    "def get_url_list2(page):\n",
    "    urls = []\n",
    "    response = get(BASEURL, headers=HEADERS)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    max_page = page + 1\n",
    "    for i in range(1,max_page):\n",
    "        url = 'https://github.com/search?p=' + str(i) + '&q=stars%3A%3E0&s=stars&type=Repositories'\n",
    "        print(f'traversing url: {url}')\n",
    "        response = get(url, headers=HEADERS)\n",
    "        # parse the fetched HTML content using a HTML parser\n",
    "        # since our page content is going to be in HTML format\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        # find the repositories container div\n",
    "        main_content = soup.find(class_='repo-list')\n",
    "        # Extract the list of repositories\n",
    "        list_of_repos = main_content.find('li', class_='repo-list-item')\n",
    "        print(list_of_repos)\n",
    "        # create a new list to put our extracted data\n",
    "        results = []\n",
    "        # Function to extract the details for each repo\n",
    "        for repo in list_of_repos:\n",
    "            # create a new repoâ€™s details dictionary\n",
    "            repository = {}\n",
    "            base_url = repo.find('href')\n",
    "            print(base_url)\n",
    "            if soup.find(attrs={'itemprop':'programmingLanguage'}):\n",
    "                language = soup.find(attrs={'itemprop':'programmingLanguage'})\n",
    "            results.append(repository)\n",
    "    return repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traversing url: https://github.com/search?p=1&q=stars%3A%3E0&s=stars&type=Repositories\n",
      "<li class=\"repo-list-item d-flex flex-column flex-md-row flex-justify-start py-4 public source\">\n",
      "<div class=\"col-12 col-md-8 pr-md-3\">\n",
      "<h3>\n",
      "<a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"stars:&gt;0\",\"result_position\":1,\"click_id\":177736533,\"result\":{\"id\":177736533,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnkxNzc3MzY1MzM=\",\"model_name\":\"Repository\",\"url\":\"https://github.com/996icu/996.ICU\"},\"client_id\":null,\"originating_request_id\":\"E6C9:50D9:10B6A4:1D437B:5CD4416D\",\"originating_url\":\"https://github.com/search?p=1&amp;q=stars%3A%3E0&amp;s=stars&amp;type=Repositories\",\"referrer\":null,\"user_id\":null}}' data-hydro-click-hmac=\"dd37b7f474ec18267c1a1737ded8c8aa8424486d3f01c9a63ca68651c8171139\" href=\"/996icu/996.ICU\">996icu/996.ICU</a>\n",
      "</h3>\n",
      "<p class=\"col-12 col-md-9 d-inline-block text-gray mb-2 pr-4\">\n",
      "        Repo for counting stars and contributing. Press F to pay respect to glorious developers.\n",
      "      </p>\n",
      "<div class=\"d-flex flex-wrap\">\n",
      "<p class=\"f6 text-gray mr-3 mb-0 mt-2\">\n",
      "          Updated <relative-time datetime=\"2019-05-09T13:35:08Z\">May 9, 2019</relative-time>\n",
      "</p>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"flex-shrink-0 col-6 col-md-4 pt-2 pr-md-3 d-flex\">\n",
      "<div class=\"text-gray flex-auto min-width-0\">\n",
      "<div class=\"mr-3\">\n",
      "<span class=\" \">\n",
      "<span class=\"repo-language-color\" style=\"background-color: #dea584\"></span>\n",
      "<span itemprop=\"programmingLanguage\">Rust</span>\n",
      "</span>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"pl-2 pl-md-0 text-right flex-auto min-width-0\">\n",
      "<a class=\"muted-link\" href=\"/996icu/996.ICU/stargazers\">\n",
      "<svg aria-label=\"star\" class=\"octicon octicon-star\" height=\"16\" role=\"img\" version=\"1.1\" viewbox=\"0 0 14 16\" width=\"14\"><path d=\"M14 6l-4.9-.64L7 1 4.9 5.36 0 6l3.6 3.26L2.67 14 7 11.67 11.33 14l-.93-4.74L14 6z\" fill-rule=\"evenodd\"></path></svg>\n",
      "          243k\n",
      "        </a>\n",
      "</div>\n",
      "</div>\n",
      "</li>\n",
      "-1\n",
      "None\n",
      "-1\n",
      "None\n",
      "-1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_url_list(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
