{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLAN\n",
    "\n",
    "- [x] Acquisition\n",
    "    - [x] Select what list of repos to scrape.\n",
    "    - [x] Get requests form the site.\n",
    "    - [x] Save responses to csv.\n",
    "- [ ] Preparation\n",
    "    - [x] Prepare the data for analysis.\n",
    "- [ ] Exploration\n",
    "    - [ ] Answer the following prompts:\n",
    "        - [ ] What are the most common words in READMEs?\n",
    "        - [ ] What does the distribution of IDFs look like for the most common words?\n",
    "        - [ ] Does the length of the README vary by language?\n",
    "        - [ ] Do different languages use a different number of unique words?\n",
    "- [ ] Modeling\n",
    "    - [ ] Transform the data for machine learning; use language to predict.\n",
    "    - [ ] Fit several models using different text repressentations.\n",
    "    - [ ] Build a function that will take in the text of a README file, and makes a prediction of language.\n",
    "- [ ] Delivery\n",
    "    - [ ] Github repo\n",
    "        - [x] This notebook.\n",
    "        - [ ] Documentation within the notebook.\n",
    "        - [ ] README file in the repo.\n",
    "        - [ ] Python scripts if applicable.\n",
    "    - [ ] Google Slides\n",
    "        - [ ] 1-2 slides only summarizing analysis.\n",
    "        - [ ] Visualizations are labeled.\n",
    "        - [ ] Geared for the general audience.\n",
    "        - [ ] Share link @ readme file and/or classroom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "from requests import get\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#Global variables holding all of our language names and additional stopwords\n",
    "LANGUAGES = ['JavaScript', 'Rust', 'C++', 'Python', 'Dart', 'Java', 'Go', 'CSS',\n",
    "             'PHP', 'TypeScript', 'Ruby', 'HTML', 'C', 'Vue', 'C#', 'Shell',\n",
    "             'Clojure', 'Objective-C', 'Swift', 'Jupyter Notebook','Vim script',\n",
    "             'Assembly', 'Kotlin', 'Dockerfile', 'TeX', 'javascript', 'rust',\n",
    "             'c++', 'python', 'dart', 'java', 'go', 'css', 'php', 'typescript',\n",
    "             'ruby', 'html', 'c', 'vue', 'c#', 'shell', 'clojure', 'objective-c',\n",
    "             'swift', 'jupyter notebook', 'vim script', 'assembly', 'kotlin',\n",
    "             'dockerfile', 'tex', 'yes', 'one', 'also', 'two', 'etc', 'please']\n",
    "\n",
    "BASEURL = 'https://github.com/search?p=1&q=stars%3A%3E0&s=stars&type=Repositories'\n",
    "HEADERS = {'User-Agent': 'Assault Potato Gun'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACQUIRE\n",
    "\n",
    "First thing that needs to happen is to get the links from the most starred github repositories.  The most complicated part here was identifying the section that had the actual urls.\n",
    "\n",
    "The `get_url_list` function does the following:\n",
    "* get a response from the BASEURL\n",
    "* set number of pages to scrape and loop through all of them\n",
    "* find the list of all the repos on the page\n",
    "* from that list find the individual list item repos\n",
    "* do a check to see if there is a language associated with the repo\n",
    "* * if no language, skip\n",
    "* loop through individual repo sections and grab the url\n",
    "* print out a list of the total valid urls scraped\n",
    "* save the resulting list of urls to a csv\n",
    "* return the urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_list(page):\n",
    "    urls = []\n",
    "    response = get(BASEURL, headers=HEADERS)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    max_page = page + 1\n",
    "    for i in range(1,max_page):\n",
    "        url = 'https://github.com/search?p=' + str(i) + '&q=stars%3A%3E0&s=stars&type=Repositories'\n",
    "        print(f'traversing url: {url}')\n",
    "        response = get(url, headers=HEADERS)\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        list_of_repos = soup.find('ul', class_='repo-list')\n",
    "        repository = list_of_repos.find_all('li', class_='repo-list-item')\n",
    "        for h in repository:\n",
    "            if h.find(attrs={'itemprop':'programmingLanguage'}):\n",
    "                a = h.find('a')\n",
    "                urls.append(a.attrs['href'])\n",
    "        time.sleep(3)\n",
    "    print(f'Scraped a total of {len(urls)} github urls.')\n",
    "    urls = ['https://github.com' + url for url in urls]\n",
    "    with open('github_urls.csv', 'w') as f:\n",
    "        ghub_urls = csv.writer(f, delimiter=',')\n",
    "        ghub_urls.writerow(urls)\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the list of urls from `get_url_list`, we need to do the following:\n",
    "* visit each of the urls\n",
    "* find the main body of the `README.md`\n",
    "* * if there is no body in the `README.md` then skip it\n",
    "* grab the readme info\n",
    "* find the prominent language and grab that as well\n",
    "* do this for all the urls in the list from the previous function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_readmes_and_languages(urls):\n",
    "    readmes = []\n",
    "    languages = []\n",
    "    for url in urls:\n",
    "        response = get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        if soup.find('div', class_='Box-body') == None:\n",
    "            continue\n",
    "        else:\n",
    "            single_readme = soup.find('div', class_='Box-body').text\n",
    "        if soup.find('span', class_='lang') == None:\n",
    "            continue\n",
    "        else:\n",
    "            repo_language = soup.find('span', class_='lang').text\n",
    "        languages.append(repo_language)\n",
    "        readmes.append(single_readme)\n",
    "    df = pd.DataFrame({'readme':readmes, 'language':languages})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a big bunch of words and languages in a dataframe, we need to do some cleanup.  These functions will do the following:\n",
    "* lowercase all the text\n",
    "* normalize the language into english-only\n",
    "* only keep words that start with letters, numbers, or whitespace\n",
    "* strip any whitespace at the start or the end of a word\n",
    "* replace any newlines with a space\n",
    "* tokenize the words\n",
    "* either stem or lemmatize the words\n",
    "* remove all standard stopwords as well as the languages and any additional stopwords that were found during exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(string):\n",
    "    \"\"\"Will lowercase, normalize, and remove anything that isn't a letter, number,\n",
    "    whitespace or single quote and return it.\"\"\"\n",
    "    clean_string = string.lower()\n",
    "    clean_string = unicodedata.normalize('NFKD', clean_string).\\\n",
    "                    encode('ascii', 'ignore').\\\n",
    "                    decode('utf-8', 'ignore')\n",
    "    clean_string = re.sub(r'[^a-z0-9\\s]', '', clean_string)\n",
    "    clean_string = clean_string.strip()\n",
    "    clean_string = re.sub(r'\\s+', ' ', clean_string)\n",
    "    return clean_string\n",
    "\n",
    "def tokenize(string, string_or_list='string'):\n",
    "    \"\"\"nltk.tokenize.ToktokTokenizer\"\"\"\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    if string_or_list == 'string':\n",
    "        return tokenizer.tokenize(string, return_str=True)\n",
    "    if string_or_list == 'list':\n",
    "        return tokenizer.tokenize(string)\n",
    "    \n",
    "def stem(string, string_or_list='string'):\n",
    "    \"\"\"Returns the stems.\"\"\"\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    stemmed_string = ' '.join(stems)\n",
    "    if string_or_list == 'list':\n",
    "        return stems\n",
    "    if string_or_list == 'string':\n",
    "        return stemmed_string\n",
    "    \n",
    "def lemmatize(string, string_or_list='string'):\n",
    "    \"\"\"Returns the lemmatized text.\"\"\"\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    lemmatized_string = ' '.join(lemmas)\n",
    "    if string_or_list == 'string':\n",
    "        # remove all single characters or numbers\n",
    "        lemmatized_string = re.sub(r'(^| ).( |$)', '', lemmatized_string)\n",
    "        lemmatized_string = re.sub(r'(^| )[0-9]( |$)', '', lemmatized_string)\n",
    "        return lemmatized_string\n",
    "    if string_or_list == 'list':\n",
    "        return lemmas\n",
    "    \n",
    "def remove_stopwords(string, string_or_list='string', extra_words=None, exclude_words=None):\n",
    "    \"\"\"Removes the stopwords from the text then returns it. Able to add or remove stopwords.\"\"\"\n",
    "    stopword_list = stopwords.words('english') #+ LANGUAGES\n",
    "    if extra_words != None:\n",
    "        for word in extra_words:\n",
    "            stopword_list.append(word)\n",
    "    if exclude_words != None:\n",
    "        for word in exclude_words:\n",
    "            stopword_list.remove(word)\n",
    "    filtered_words = [word for word in string.split() if word not in stopword_list]\n",
    "    filtered_string = ' '.join(filtered_words)\n",
    "    if string_or_list == 'string':\n",
    "        return filtered_string\n",
    "    if string_or_list == 'list':\n",
    "        return filtered_words\n",
    "    \n",
    "def remove_languages(string, string_or_list='string', extra_words=None, exclude_words=None):\n",
    "    \"\"\"Removes the stopwords from the text then returns it. Able to add or remove stopwords.\"\"\"\n",
    "    stopword_list = stopwords.words('english') #+ languages\n",
    "    if extra_words != None:\n",
    "        for word in extra_words:\n",
    "            stopword_list.append(word)\n",
    "    if exclude_words != None:\n",
    "        for word in exclude_words:\n",
    "            stopword_list.remove(word)\n",
    "    filtered_words = [word for word in string.split() if word not in stopword_list]\n",
    "    filtered_string = ' '.join(filtered_words)\n",
    "    if string_or_list == 'string':\n",
    "        return filtered_string\n",
    "    if string_or_list == 'list':\n",
    "        return filtered_words\n",
    "\n",
    "# fancy pipe function\n",
    "def pipe(v, *fns):\n",
    "    return reduce(lambda x, f: f(x), fns, v)\n",
    "\n",
    "def readme_lem(text):\n",
    "    return pipe(text, basic_clean, tokenize, remove_stopwords, lemmatize)\n",
    "\n",
    "def readme_stem(text):\n",
    "    return pipe(text, basic_clean, tokenize, remove_stopwords, stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we already have the urls saved, we don't need to re-scrape them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('github_urls1.csv') as f:\n",
    "    urls = f.readlines()\n",
    "urls = urls[0].split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking to make sure the size of the list of urls makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping the information we need from GitHub with our function and storing it in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grab_readmes_and_languages(urls)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book2.csv                github_data.csv          github_urls5.csv\r\n",
      "DF_tree                  github_data.html         github_urls6.csv\r\n",
      "DF_tree.pdf              github_urls.csv          mz_working_nb.ipynb\r\n",
      "README.md                github_urls1.csv         nlp_github_project.ipynb\r\n",
      "full_719.csv             github_urls3.csv         nlp_project_orion.ipynb\r\n",
      "git_hub.csv              github_urls4.csv         personal_nb_mz.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
